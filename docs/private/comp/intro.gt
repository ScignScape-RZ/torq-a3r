

Language as computation (or not): the case of thematic relations

Just as "the mind is a computer" might be construed as suggestive metaphor or as something explanatorily more than that, so too "language as computation" tropes sport varying degrees of literal intent.  On the one extreme, we might imagine that the meaning of a sentence can be calculated via deterministic processes not especially different than solving mathematical equations.  At the opposite extreme, language's contextuality and nuance could be deemed fundamentally incompatible with any mechanistic metaphors.  I would guess most linguists' and philosophers' of language intuitions fall somewhere in between.  

Finding merit in computational metaphors does not commit us to paradigms wherein linguistic content as a whole reveals that level of formal transparency.  We might insist that once language qua abstract system becomes concretized in real-world dialogs and social interactions a panoply of additional cognitive and interpersonal details become woven into the communicative fabric.  Deixis, anaphoric resolution, sarcasm, oblique imperatives, proper names and other referring expressions deployed as presumptively familiar to everyone listening, etc., all rely on social competences that most likely cannot be modeled via grammar or semantics.  But speculations as to language's "computability" can be entertained differently vis-a-vis syntax, semantics, and pragmatics.  Even if cognitive simulations appear to make some progress in formalizing pragmatic conventions, for example, the genre of formalisms employed would likely be importantly different than those on semantic and syntactic registers.  Computational metaphors, in short, can have some validity even if they are limited in scope, relative to the totality of human communication.
That rather noncommittal take, of course, leaves many questions open.  What is the border between those facets of language that can and cannot be usefully approproached from computational perspectives?  Why -- i.e., what cognitive and/or structuralist principles -- place that border wherever it is?  How do the two sides connect together in the mind?  That is, how do your language-processing brains juggle manipulations on a menagerie of communicative subsystems some of which operate via computably simulatable rules and soome which do not?

My aim here is mostly to explore some of these questions in the specific context of thematic relations, alongside such related topics as theta grids, semantic roles, ditransitive constructions, the appropriateness of prepositional-clause to theta-role transpositions (e.g., "dative shift") and verb-alignment morphosyntax (as in tense, aspect, mood, and evidentiality, or "TAME").  Alluding to the philosophically uncertain status of language-as-computation (and indeed of mind-as-computer) metaphors will allow me to leverage these linguistic theories toward a more general argument about how we understand language in the context for more general situational and social-cooperative cognition.  So, toward the end of this paper, I will branch out from narrow semantic/syntactic discussions to more speculative themes.

Symmetrically, then, at the start of the paper, I will also step outside linguistic proper and make some preliminary observations vis-a-vis computational metaphors in general.  Specifically, so as to have a somewhat rigorous orientation for assessing and/or cashing in such metaphors, I will try to frame the topic philosophically: what, exactly, is a computation in the first place?  Drawing on (a little) mathematics and some programming-language theory (compiler theory and Software Language Engineering) I will argue that this question is more multi-faceted than one might suspect at first.  A sufficiently well-rounded model of computational systems/processes can clarify the connections between formal/programming and natural/human languages, which can set up a disciplinary groundwork to endow "language as computation" metaphors with a degree of structural substance and explanatory purpose.

      If we begin from a purely mathematical perspective, the notion of computation builds off of functions from a domain to a codomain, such as integers to integers or reals to reals; from θ to sin(θ), for instance.  Multiple expressions can be evaluated in sequence: the area of a circle segment, for instance, with radius r and sector-angle θ is (r2 / 2) [θ - sin θ]; i.e., start with the sin(θ) mapping and then calculate a subtraction, a square, a multiplication, and a division (by a factor of 2), in a fixed order.  Since there are 2 non-constant parameters in this formula, it describes in its own right a function on two numbers (its domain is an ordered pair of reals, RxR; its codomain is just R).  
Assuming we have the capability of notating some collection of preliminary functions (sine, addition, multiplication, etc.) we can describe a infinite variety of derived functions, each of which has a domain and codomain that can be assembled from those of the preliminary functions (various tuples, cross-products, subspaces, and other constructions on those preliminary sets).  

A canonical logicomathematical device for modeling derived computations is a *term algebra*.  A term algebra is an "algebra" in the sense that it describes operations which transform one term to another: in particular, a single constant or variable in a term may be replaced by a different term, yielding a nested hierarchy analogous to linguistic parse-trees.  Moreover, extending a term by one additional element yields a new term.  Each term contains one function (or a symbol or term that can be interpreted as identifying a function, or else evaluated to a function) and zero or more "argument" terms.  A term algebra is assumed to include some collection of symbols that designate functions, constants, or variables, with a single constant or variable understood as a term in its own right.  The set of all possible terms is constructed as the "closure" of a set of primitive terms under the term-building operations (by analogy, the set of natural numbers can be characterized as the cloure of a basis set such as zero and one under the operation of addition: the positive integers Z+ are the smallest set such as the sum of any two positive integers is in the set).  The axioms of term algebra are inspired by the concept of "applying" functions to parameters (under this interpretation terms are also called "applicative structures"), but the formal definition of terms makes no explicit use of this interpretation; the distinction of functions, constants, and variables is just an artifact for stipulating which structural arrangements of these constituent elements are a valid term.  From a linguistic perspective, term algebras can be seen as defining a class of formal languages, by filtering out a restricted set of structures as valid strings.  According to the Chomsky-Stultenberger Representation Theorem, a formal language adequate for modeling term algebras can be decomposed into a combination of two simpler languages, one for tuple-information and one essentially for "balancing parentheses".

In logic and the foundations of mathematics, term algebras form the basis of lambda calculus, with the notion that non-constant variables are "lambda-abstracted", such that any particular value they might have is abstracted so as to yield a non-specific formula/expression.  A "variable" can be assigned a specific value on specific occasions, but in general it is associated with a domain of potential concrete values rather than a single concrete value (unlike a constant such as `pi`).  A formula containing a lambda-abstracted symbol similar accepts parameters from a domain-set rather than expressing a specific value.  This is a pretty intuitive and easily-definable account of "formulas" or "equations", which might seem sufficiently self-obvious as to preclude the need for detailed mathematical constructions.  Formalizing such familiar concepts in terms of term algebras and lambda calculii, however, establishes a structural framework whose merits become more obvious in contexts such as recursive function theory and the study of computable functions (questions such as, given a preiminary basis of predefined functions, topologically describing the space of functions, with given domains and codomains, that can be evaluated by a Turing-compatible computer in guaranteed finite time; and likewise discovering properties of functions that are *not* computable).

A term whose structure is an n-ary function, together with the proper count $n$ of constant-valued arguments, can be *interpreted* as the result of applying that function to those values.  In lambda calculus, such evaluation is called "beta-reduction", and nested terms describe sequences wherein one beta reduction permits the replacement of a subterm with a single value, which in turn may allow some parent term to itself get evaluated, and so on iteratively.   A term is well-formed if a chain of such reductions eventually results in one sole term that generates a final value.  As such, one semantic reading of a term algebra can treat each term as a schema for coordinating a sequence of intermediate function-applications so as to compute the value of a formula, once all abstracted variables are assigned concrete values.

It is obvious that such a framework sets the stage for a model of computation in general; however, there are several different mechanisms by which the intended semantic interpretation might be codified.  One possibility is to generalize one's definition of terms to include tuples of distinct terms, with the intention of reading tuples as instructions to substitute the trailing terms for unbound variables in the lead term.  Beta-reduction thereby becomes a partial lattice on the space of terms in this larger sense, which overall captures the idea that abstract terms are *potentials* to obtain specific values.  Another option is to construe beta-reduction as an eroding process, a kind of graph-transform system (taking trees as a special class of graphs) wherein each mapping of a term's full complement of unbound variables to a tuple of concrete values gets associated with a graph-transform sequences, governed by the rule that terms structurally expression function-application (i.e., those with no nested subterns) may be replaced by single-value terms.  Still a third idea is to treat terms as state machines: in their "raw" state with some tuple of unbound variables, they define generic formulae; substituting concrete values generates alternative states such as nested terms are paired with specific values within a relevant codomain (in other words, certain states of terms involve association with the elements of that set, capturing the idea that terms evaluate to results when all variables are bound to values), and beta-reduction observes a "propagation" of state-changes such that successive terms, from lower to higher on the tree-hierarchy, enter the state of having such fixed value-association.                 


Someone sent a package for Dave: specific Dave
Coffee for Dave (barista): any Dave
