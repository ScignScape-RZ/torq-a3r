https://en.wikipedia.org/wiki/Determiner_phrase

https://rucore.libraries.rutgers.edu/rutgers-lib/30277/PDF/1/play/
EVIDENTIALITY AND THE STRUCTURE OF SPEECH ACTS

https://www.sas.rochester.edu/lin/sgrimm/publications/Grimm_Morphology_Case.pdf
Semantics of Case
Scott Grimm

https://en.wikipedia.org/wiki/Thematic_relation

https://books.google.com/books?id=_FfPDwAAQBAJ&pg=PA1&source=kp_read_button&hl=en&newbks=1&newbks_redir=0#v=onepage&q&f=false


https://www.ling.upenn.edu/~beatrice/syntax-textbook-2nd-ed/SKD_7_Case_theory.pdf


I will sing you a song
Now I'll recite you some poems
write you some lyrics


find the baby some clothes
I'm gonna catch me some fish
let's allocate this project plenty of time


I plan to paint you the house for wintertime
I reckon I'll paint you the house for when your parents come











he got ready to leave
he got up to leave
he stood up to leave
he stood, ready to leave

ready to leave, he stood
*ready to leave, he get
*up to leave, he get

he was getting angry at the media
he was getting increasingly angry at the media
he was increasingly getting angry at the media
angry at the media, he was getting 



look out  vs.  watch out  vs.  gaze out
CS theory  
lexicalization is a matter of degree

if the structural difference between two parse-graphs depends on whether or not we read a phrase as de facto a compound verb, and moreover if lexicalization is not all-or-nothing but shades between a fully conventionalized lexeme and an entirely novel construction -- with many cases lying in-between -- then inevitably parse-graph divergence also has to be seen as a matter of degree.  Two graphs can coexist as plausible breakdowns in the same way that (and as a consequence of how) a verb phrase must be assessed as partly a quasi-word and partly a not-fully-conventionalized expression.


John waved out 

How did you know we were here?
I saw John staring.  Out the window.
 
I saw John staring out the window.


Where did you climb?
I climbed up to the roof.

How did you get to the roof?
I climbed up (to the roof).

How were you able to see over those trees?
I climbed.  Up to the roof.



How did you get here?
I climbed up (a ladder).

Where did you climb?
(I climbed) up a ladder.

How were you able to see everything?
I climbed.  Up a ladder.


If you gaze out the window, it could be that "out the window" is where you're gazing, but it could also be that you're "gazing out" -- by analogy to "looking through (a telescope/camera/pinhole)".  You could "climb out the window" as in climbing out *through" whe window, and also as in you're climbing, and it so happends that "out the window" is where you've climbed.




John assumes Biden will step down.
The flower assumes it will rain.


People at the top get complacent.  A mountain assumes the valley will be there foreoever.
People think they're indispensable.  A bridge assumes the river will be there foreoever.


We're going to visit grandma.
We're going to visit the leadership group.


The White House has become increasingly belligerent toward China.
Wall Street reacted favorably to the midterm elections.
Brussels and London are negotiating again.

type coercion.

gradations in granularity between grammatical categories and semantic types

A wooden bat breaks more easily than a metal one.
*A red bat breaks more easily than a metal one.



A French-suited deck is easier to shuffle than tarot cards.
?A French-suited deck is easier to shuffle than laminated cards.

a no-trump bid/contract/lead/strategy/defense

?a no-trump hour  (plausible if many or all contracts won during some hour playing bridge were no-trump)

??a no-trump moon  (plausible perhaps, allowing for figurative interpretations, from someone who has a tendency to be dealth hands suited to no-trump bids when the moon has a certain appearance; perhaps a joke alluding to superstitutions)


a baseball fan/player/coach/team/journalist/stadium/bat/glove/addicionado

a baseball city (implies that baseball is a popular or the most popular sport)
a baseball century (implies discussing events from a 100-year periond related to baseball)
?a baseball restaurant (plausible for a restaurant with a distinctive baseball theme -- framed meeorabilia and so forth)
??a baseball presidency (plausible if suggesting that some president accomplished nothing other than watching baseball)



 
Two themes: how to charactize the inacceptability of phrases which do not appear to repsect contextual framing; and how to model the coercion effects wherein contextual framing imposes restrictions on interpretation that would not exist in general.  For instance, "visited grandma" is narrower than just "spent time with" or "chatted with" (or even "visited with") and also narower than similar designations of place:

We visited with grandma when she came here last week 
* we visited grandma when she came here last week
We drove by grandma on the way to school.  Later we learned she wasn't home.
* we visited grandma but she wasn't home.

"Visit" appears to coerce "Grandma" to a place (e.g., her home), but with the stipulation or narrowing that she must actually be there; thus "grandma" is not just a metonymic substitute for "grandma's home" the way it could be with "drove by", or with "we drove by the Mets on the way to the airport" (referring to the baseball stadium where they play) or "we walked past the dean to get to your office" (referring to the dean's office; contrast "we sneaked past the dean to get to your office"). "Visit" appears to impose a more granular and information-bearing contextual frame than "drive by", and likeise "sneak" versus "walk".

Such framing effects can operate on different levesl: among the coarsest grammatical categories, so that the only explicit interpretive requirement is that words are treated as nouns, verbs, adjectives, and so on relative to the proper parse-graph ("baseball" becomes an adjective in "baseball bat", etc.); at an intermediate "ontological" level differentating places from objects, inanimate objects from sentient ones, etc. (you can only "sneak past" something capable of observing and of forming opinions); and at a fine-grained lexical level (see "no-trump" vis-a-vis card games).  To what extent should we consider the cognitive mechanisms behind contextual framing as similar across all three levels, versus stipulating that each level should be theorized separately?

This question has two dimensions: whether it is most appropriate or fortuitous to *model* different levels of contextual framing as similar or distinct; and whether the actual cognitive/neurological processes carried out in the mind are similar or distinct.  These dimensions are somewhat separate, because in the absense of neurocognitive evidence it is a matter of speculation to what degree framing obeys similar intellectual mechanisms at different levels.  As such, preferring to unify the levels under a single theoretical umbrella depends more on which models feel like they have the most explanatory merit or structural propriety for further research/analysis.  [apart from neurological data, computational evidence that NLP engines which employ similar or, conversely, divergent algorithms for computing within contextual frames at different levels might also give empirical feedback.  I am not aware however of analyses which have compared NLP technology along such lines.]

In general, "type-level" beta-reduction seems more compatible with assumptions that contextual framing is properly studied across multiple scales of granularity.  This is because beta-reduction as "type checking" works essentially the same irregardless of the paucity or numerosity of types over which nodes can quantify.  Moreover, there seems to be a gradation between "macro-types" (grammatical categories) and "meso-types" (ontological regions, in effect): consider singular/plural, or mass/count.  To what extent are mass and count nouns distinct grammatical categories, or are their differences more a semantic matter driven by situational differences (perceiving/experiencing/cognitizing some extent as mass or count representing an pragmatic stance toward those surroundings, motivated by extra-linguistic considerations).  Leveraing beta-reduction style models to span different levels of contextual granularity (of course the notion and specific details of "beta-reduction" don't need to be explicitly stated as such) helps acknolwedge that it is hard to draw a crisp line between the kind of contextual framing imposed by grammatical and by ontological categories.

On the other hand, I do not seek to foreclose analyses which highlight differences between context-framing levels.  The rationale for modeling context (in the relevant sense) via beta-reduction is that such an analogy implies a reasonably formal/deterministic process which concludes that only certain interpretations of a phrase's constituents aggregate to a coherent whole.  "Visit the leadership group" only makes sense if we can read the group as a *place* (e.g., where they habitually gather -- ruling out, say, soliciting opinions for email or holding a conference call), with place-ness being part (though only part) of the conceptual matrix through which we figure the referent and signified for, e.g., "leadership group" as a phrase.  Moreover, context-framing constraints propagate from nested phrases outward: 

I visited the leadership group and took a cab back. [no need to specify back from where]
?I consulted with the leadership group and took a cab back. [the second half of the sentence supplies new information not presumptively included in the first part]
*I studied the leadership group and took a cab back. 
 
?I visited the newspaper yesterday and bought a copy today (plausible for someone who might have some special interest in learning about some newspaper, e.g. a prospective employee).


Such propagation effects imply that types/categories need to "fold up", kind of by analogy to a cardboard box that can lie flat only when the flaps are collapsed in the right directions and order.  Novice box-folders tend to backtrack before getting the sequence right.  By analogy, perhaps our minds scan through diverse "folding patterns" measuring whether phrases collapse to a lucid proposition given specific lexical and part-of-speech interpretations of individual words or world-like phrases.  Computer programming languages work the same way when it comes to resolving overloaded procedures in a generic-programming context (see section 1).

This all might seem like a lot of theoretical content to make the fairly obvious point that phrase-heirarchies cannot be assembled (or disassembled) in an incoherent manner.  Semantic and situational (as well as syntactic) reasonableness eliminate many hypothetical parse-trees, hopefully leaving only one still standing.  Notice, though, that insofar as contextual constraints on phrase-formation operate at differerent granularity levels, the "elimination" is driven by multiple levels concurrently (or at least each contributing their own qualifications); we therefore need a framework to study how the different levels interoperate.  In other words, assessing candidate parse-interpretations is a dynamic process steered by different granularity levels (perhaps in sequence, perhaps interleaved, perhaps cooperating); the propagatory facets of beta-reduction perhaps make this a more intuitive model for such dynamic processing than "static" constituency diagrams, such as those of X-bar theory.  

Furthermore, elucidating a sentence's parse-graph is only one step in the comprehension process, and addressees still need to complete lexical details and resolve references/anaphora.  If we (perhaps as a simplifying abstraction) view parsing and overall comprehension as two successive stages, the amount of information yielded by the first phase can vary from case to case.  For instance, in "visit the newspaper" we have data to the effect that "the newspaper" must be read as a place-designation, from which it naturally follows that this phrase designates a building/location the paper occupies.

If we picture parsing as "beta-reduction", the idea would be to treat the graph (one which is tree-like, but may contain lateral edges as well such as pronouns-to-antecedents) as comprised of nodes "labeled" with categorical or type attributions that bear at least some information.  Only certain such "labeling" would survive the eliminative process whereby mismatched part-of-speech (or finer) attributions fail to fold up to a coherent proposition-like whole.  So the labeling serves that eliminative purpose.  But simultaneously, the labeling produces information that remains in effect during subsequent comprehension: e.g., data such as "the newspaper" needing to reference a place (and not, say, an object).  The more granular the available set of node "labels", the more information leaks out from the first phase onto the second.

In short, the processes which converge on our realization that a certain parse-graph is correct for a given sentence also result in criteria attached to individual words or phrases that then serve as a starting point for deciding what the sentence actually *means*.  More to the point, the *quantity* of this data would seem to be larger or smaller from sentence to another.  In some cases, we have lexical and/or morphological evidence of singular/plural or mass/count.  In other sentence, "deep case" or semantic-role considerations add extra details that are prerequisite for parse-graphs being admissible, thereby yielding a more granular array of data left over from the parsing "stage".  A useful model of these phenomena should thereby present a structural framework which adapts to diffrent amounts of information being provided based on different levels of specificity with respect to what content becomes associated with graph-node.  This works with a beta-reduction account because the reduction rules themselves adapt to the cardinality of sets over which node-labels quantify.  For some sentences only rudimentary part-of-speech constraints have any effect; for others, there is explicit contextual framing at an intermediate ("meso-type" or thematic-relation) level (e.g., how "assume" implies a sentient subject and a proposition-like object).  Arguably, in still further cases there are lexical considerations which impose even tighter contexts (and thus more detailed after-parse information), since some words force a specific topic on their neighbors (see "no-trump defense").       

Beta-reduction thereby becomes a useful metaphor and structural precis; it doesn't need to be a rigorous formal simulation of linguistic processing (as compared to the case with implementing compilers for computer languages, which have a mathematical substrate: in the case of human language we do not need to picture the mind itself as some sort of lambda-calculus machine).  However, the "type-based" account of beta reduction captures only partial desiderata toward understanding sentences as a whole; in this sense the theories differs from ones I outlined earlier in the section, based on (e.g.) Category Theory or Predicate Logic, wherein the entirety of meaning (or at least of semantics, allowing for pragmatic resolving of deictic references and the like) is computed via a certain combinatorial process (for which beta-reduction would be a reasonable synopsis).  As a semantic theory the account presented thus far obviously then goes less far than those others.  I will now try to close some of that gap.

--

Beta reduction models how parsing constraints narrow the space of plausible parse-graphs to a few (potentially superimposed) which, assuming an unambiguous sentence, either yield similar interpretations for the overall sentence (recall the "look out" and "climb up" examples) or are subject to additional selective possibilities so as to converge on a single reading.  I have not addressed, however, the path which our minds take toward understanding a sentence in full detail, which would in general exhibit a greater degree of comprehension than parsing alone.  I might have a reasonable chance at guessing the syntactic properties of a sentence containing unfamilair words -- perhaps aided by morphological observations, such as gerunds on verbs or plurals on nounds -- but remain puzzled as to what the whole thing actually means.  There is obviously *some* gap between syntax and semantics/pragmatics.

To explore it, I will start with the distinction between "theta roles" and "thematic relations" which I touched on earlier.  We assume that elements of a sentence can be classified in terms of the details they contribute to a verb -- i.e., through the verb we profile some event, state, or process; and surrounding words add information so as to characterize that events/states/processes more fully, reletive to how the speaker perceives them.  The "TAME" criteria cover various facets of verb detail: tense and aspect concern termporal matters, whereas evidentials report on how the speaker came to observe or believe the event/state/process signified.  These represent one layer of specifity pertientn to the verb's usage in context.

Subject, object, and direct object are a (probably) different layer or "zone".  They appear to be intrinsic to the verb, in that -- modulo competing word-senses, e.g. "climb a mountain" versus intransitive "climb" qua activity -- verbs' meanings carry built-in expectations of subjects (even if "null" subjects like "it's raining", or commands' addressees) and, if applicable, direct then indirect objects.  To *give*, say, connotes a conceptual matrix which demands the fulfillment of subject (the giver), direct object (the thing given), and indirect object (recipient).  Absense of any of these forces us to read the constituents as obvious in context and therefore not explicitly stated (in "give to charity" it is understood that one is giving money).  To "introduce" implies a ditransitive structure separating an introducer, someone/thing introduced, and some sort of audience; absense of some part forces us either to read it contextually or to re-evaluate the lexical details, perhaps concluding that "introduced" is being employed in just the transitive sense (as in "she introduced the speaker").  Even here, though, one can debate whether the introduction's "audience" is unstated but implicit ("introduce a speaker" implies that there is a group of people whom the speaker will soon address) or whether there are two basically distinct word-senses for "introduce", one of which has an indirect object and one which does not.  In some cases the different-word-sense analysis is more straightforward: consider "I drew him" versus "I drew him a picture" (it is harder to read the "single-" transitive "draw" as actually a ditransitive verb with an indirect object understood by default when not stated, as compared to, say, "introduce").

These subject/object considerations can be differentiated from TAME in that the former expectations are generically associated with the verb, in an abstract way that can be specified in a dictionary, whereas TAME depends on dialogic context.  It is an *instance* or *token* of a verb that has temporal bounding/specificity (or, alternatively, open-endedness) and which permits different evidential markers depending on the speaker's epistemic position.  By contrast, a verb's property of requiring, say, a direct object is not situation-specific (at least insofar as we are inclined to classify the optionality of a direct obejct in some cases as the verb being partially polysemous, rather than as a single meaning which can get grounded sometimes with a direct object and sometimes without).  The contrast between TAME and "theta roles" may not be entirely transparent, but it seems reasonable to argue that TAME data are *only* operative when we concern a specific grounded verb-instance, whereas "theta roles" are abstract slots built in to the meaning of a verb.

This language implicitly assumes that we can distinguish theta roles (where are available for morphosyntactic/inflectional marking) from more general "thematic relations".  In English, of course, the indirect object can be introduced via a phrase ("I gave the book to him") but also positionally, implying the presence of an (unmarked) case ("I gave him the book").  The second form is a short-cut which suggests (and is possible because) a certain indirect-object relation is canonically associated with the corresponding verbs.  Other such relations are less conventional and therefore do not work except when phrasally articulated:

I opened the bottle with a corkscrew.
*I opened corkscrew the bottle.
?I corkscrewed open the bottle.


I signed the book for him.
?I signed him the book.

And "I read him the book" means *for* him, not *to* him.       


In theories that differentiate theta roles from thematic relations (and this is not ubiquitous), the contrast is apparently driven by whether the relevant semantic roles are *necessary* for the verb to yield a complete idea.  This "I sent him the book" could be followed by further information (yesterday; by post; for you; from Manhattan) but the sentence is complete (enough to be coherent) by itself.  So we might distinghuish content which is necessary vis-a-vis the verb *instance* (TAME) from that necessary by lights of the verb's overall meaning (the cases it *must* support) and also from that which is somehow "optional".  I suggest this is analogous to a system of "zones" that organize sentence material in terms of their mandate's extent when fully grounding the relevant verb.  TAME details might be called the "innermost" zone because these are most likely to be marked on the verb itself (e.g., through conjugation patterns) whereas theta roles, if they are marked at all (as opposed to introduced via residual phrases) would depending on declensions on nouns, not the actual verbs.  "Optional" extra descriptions, such as where something happened, or how (etc.) act as a kind of "outermost" zone.

Such a tripartite breakdown could be a good start, but consider again I question I mentioned earlier, namely the status of place nouns as the destination in verb of motion, such as "I climbed up to the roof".  Should we read "to the roof" as in a sense a direct object -- an answer to the question "where did you climb?" -- or is "I climbed up" a complete thought in and of itself?  In short, does "to the roof" here belong to the outermost zome (optional) or to an intermediary zone?

Arguably, this kind of question is relative to the foci of the person observing the verb's action (or inaction).  Suppose 

She turned onto Atlantic Avenue.

Presumably the speaker is reporting the action of a driver (or maybe a bike rider, perhaps a pedestrian) who is now following Atlantic Avenue (but was not previously).  The interest seems to be in where that person is going now; by extention, then, on the practical ramification of the fact of making that turn (she's going in a different direction than if she'd stayed straight).  The specific seconds where tge actual turning occurred are less centralized; our mental attention is zoomed outward somewhat.  These attitudinal details appear to reinforce the reading that "turn onto" is functioning as a compound verb, and "Atlantic Avenue" as a direct object (insofar as "turn onto" evinces a conceptual matrix that requires specifying a new direction and location than before).

Conversely, suppose we read (*) as if the verb itself were just "turn", and then "onto Atlantic Avenue" becomes supplemented but inessential detailing.  This gloss is only reasonable if the speaker's attention were zoomed in more narrowly onto the act of turning.  Compare a statement about some bike rider:

He fell onto a railing, bruising his shoulder.
He fell, bruising his shoulder.

In the second case "fall" is the main event, and the speaker apparently wants to explain how that mishap affected the rider in hurting his shoulder.  Whereas [] pans attention out slightly to emphasize that the relevant event was not just a falling, but *falling onto* something (the fact that it was a presumably-hard railing implied to be partly why his shoulder got injured).  Here "the railing" could be read as a direct object for "fall onto" or, alternatively, "onto the railing" as the locative container for an intransitive "fall".


Examples like these imply that questions over whether certain details are "essential" to a verb depend on the speakers' point of view.  That's not necessarily a bad outcome, but it does complicate the idea that "intermediate zone" semantic roles are intrinsic to the verb's (in its relevant word-sense) "dictionary meaning", in contrast to tense/aspect/mode/evidentiality which come into play with the specificity of the verb's *grounding* (it is only when instantiated that a verb suggests some temporal extent, say).  

Along similar lines, observe also that inflectional or positional marking reinforces conceptual essentiality -- in English, details have to be supplied through (often preposition-led) phrases exce[t for canonical relations such as "give" and its indirect object ("I gave him the book").  But conceptual nonoptionality by itself does not guarantee that compressed non-phrasal constructions are permissable.  For instance, suppose someone says "I brought K'eyush to the store" (so he could select his own treats/because they allow dogs/because the dog-sitter's on vacation or whatever).  Now certainly no English speaker would balk at someone saying just "I brought K'eyush", but we should probably read that as simply implying an unstated location/indirect-object, as in by default "I brought K'eyush *here*".  The verb "bring", that is, probably always has a 3-part conceptual matrix where the "location" part (whatever place the direct object is brought *to*) defaults to the current place where the conversation is happening (similar to examples like "I rbought coffee", or "there's a lot of people", which in turn we take to mean a lot of people *here*).

But despite that conceptual mandate (the location/indirect object must be present) we may not transpose sentences by analogy to "I gave him the book":

*I brought the store K'eyush

... although the compressed form works sometimes:

I brought K'eyush some treats.
I brought him a book.

These seem admissible perhaps because the recipient is not only a *location* but also a *benefactor*.  If anything, though, the question of on whose behalf something is brought seems less non-eliminable than *where* it was brought.  Someone can certianly say "I brought some treats", implying (as I've argued) they've brough the treats *here*; and that is a complete idea, even though they could well add on "for K'eyush".  


[https://gawron.sdsu.edu/syntax/course_core/final_13/theta_grids.pdf]


The salient conceptual matrix for "bring" seems to be "*someone* brings *somthing* to *some place"" but, on reflection, perhaps we almost always infer a notion of beneficiary or recipient, even if unstated.  For instance, "I've brought coffee" implies "cofee for us/everyone (to drink)" unless the context compels a reading where they brought the coffee only for themselves (as in "what did you bring for your lunch?").  Indeed, the "recipient" role seems entrenched to the point where that thematic relation, and not location per se, gets slotted in for the condensed construction:

I brought the house some dinner (i.e., dinner *for* the people *in* the house)
I brought paint for the house.
*I brought the house paint.

The first example actually shows a type-coercion effect along the lines covered above, where "the house" must be read as a sentient beneficiary and therefore we take it as obliquely referring to people *in* the house.  It is not enough for there to be a reading where the recipient somehow benefits or is improved, as with bringing paint; we cannot transpose (2) to (3) in parrallel to these:

I brought some paint for our young artist.
I brought our young artist some paint.

The recipient in () and () can presumably *experience* the benefits accured by whatever is brought, whereas "the house" might look better with a fresh coat of paint but would not experience itself as such.

Perhaps, then, "bring" actually has a 4-part conceptual grid with a subject and direct object and then *two* essential further roles, that of a *place* where the latter is brought and that of a *recipient*.  Notice that both of these roles might be ineliminable; examples where notions of place are absent seem to be figurative metaphors rather than down-to-earth instances of a "bring" schema:

You brought us all happiness
Exercise brought him good health
Fate brought him good luck

These seem to play off of a metaphor of one's emotions or selfhood as a "place" -- perhaps figured as an interior, spiritual location, or perhaps as a target/destination -- rather than removing place from the schema entirely.  See:

I was in a dark place.
I'm in a better place now.
At the end of this road lies sorrow.
I'm on my way to happiness.

Assuming both *recipient* and *place* are intrinsic parts of the "bring" schema, then it makes sense that the "brought K'eyush treats" pattern would be the one marked by word order rather than a nested clause, because the "place" component could be inferred from context as either the setting for the current dialog and/or wherever the recipient is/was.  There is an obvious default of "here" on the location slot but no similarly obvious default for the recipient slot.

This part of my analysis is certain unoriginal.  I'm not trying to make a big deal about the existence of 4-part thematic-relation grids or tendencies guiding when semantic roles' participants can be introduced via marked positions (generically the indirect object) rather than prepositional phrases (since I'm only considering English I won't address case marking via morphology or inflection, but of course where those are present we could extend beyond positional marking alone).  What I want to emphasize instead is how these sorts of examples complicate intuitions that theta roles are distinguished from "optional" thematic relations in terms of conceptual requirements for a complete idea, and/or in terms of trsanspositions from preopositional clauses to positional marking.  

The notion that some thematic relations are essential and some aren't evidently derives from our intuition that a proper verb-construction can yield a complete idea, even if there also exists plenty of options for refining it.  The "I brought coffee" is (arguably) a coherent proposition, whereas "I brought coffee for everyone" adds more detail but has some content which is not absolutely necessarily, at least if logical satisfiability is the main criteria (we can strip sentences down to the minimalest core having identifiable truth conditions).  Of course, the truth conditions of "I brought coffee for everyone" and "I brought coffee for the boss" are different, but we might say that both sentences have an overlapping core; "I brought coffee" by itself does indeed have determinate truth-conditions (basically, if the speaker can show the dark liquid, they're telling the truth).  The *recipient* and *place* roles may still be in effect, but with defaults: bringing coffee (barring some discourse to the contraey) *here* and *for everyone*.

All that being said, however, looking for "complete ideas" may not be enough once we factor in dialog context.  We could have an exchange like this:

I brought some cold beer.
You sure it's still cold?
I brought the beer in an ice-chest cooler.

Given my discussion up to here, the "in an ice-chest cooler" specification would be in some sense 
"inessential" (because *how* the bringing was enacted -- the means or vehicle -- is not part of the perhaps 4-part schema that is conceptually *necessary* for "bring").  In context, however, the other person(s) already knows the speaker had brought the beer, so it is really only that emendation that introduces new content.  Since the whole point of () is to report that the beers are staying cold, it seems counterintuitive to propose that the key phrase carrying that detail ("in an ice-chest cooler") is relatively less important.  Assuming we read that phrase as a "deep-case" instrumentive, it seems that in this specific context the *how* of "bring" is no less significant than the *where* and *for whom*.

To unpack these issues, we might say that certain thematic relations are "situationally" essential; i.e., that, against the backdrop of the specific situation which the speakers seeks to emphasize, certain participants in the overall verb-detail complex are especially important.  But this is situation-specific, rather than a general phenomenon that could be described dictionary-like.  Such is a distinct notion of essentiality, divergent both from details that are unavoidable for any verb-grounding (i.e., the TAME facets) and from the schematic status of conceptual matrices that are supported by a verb in general.  Semantic roles that are schematically optional might nonetheless be intrinsic to a sentence's purpose in its specific situational context.

Overall, then, the idea of an "intermediate zone" -- vis-a-vis the binding of words and phrases that add information to the profiled verb -- actually seems to span multiple "layers": constituents that are explicitly marked (e.g., as subject, direct object, or indirect object); those that are schematically intrinsic to the verb-sense and therefore must be present (albeit allowing for "defaults") even if there is no way to introduce them except via supplemental (typically prepositional) phrases; and details that are "situationally" intrinsic to the speaker's intent even if they would be deemed optional on more generic schematic/dictionary conceptualizations alone.  These various genrea of semantic roles and/or markings "propagate" from the verb outward (where inflectional variation like "brought" as past tense represent the innermost layer of detailing, at least vis-a-vis how it is grammaticalized, and supplemental phrases that seem to be tacked on to sentences as after-thoughts comprising the outermost layer).

Returning to "beta-reduction modulo topology", then, this demarcation of different role "zones" add structure to the parse-graphs over which beta-reductions fold up.  Phrases belong to various zones, in ways that are not apparent from part-of-speech attributions alone.  For example, in "I heard that you brought treats from home for K'eyush in your purse", the "I heard that" phrase connects to the verb as an evidential, whereas "for K'eyush" is part of "bring"'s definitional conceptual schema, and "in your purse" is effectively instrumentive and supplemental (and "from home" the equivalent of ablative case, maybe?), except for situation-specific contexts such as the cooler for the cold beer, where seemingly-residual "deep cases" (ablative, instrumentive, etc.) are atypically foregrounded.  Although structurally we can still employ the paradigm wherein types and/or grammatical categories (at some level of granularity) beta-reduce up from leaves to roots, we now have further parameters insofar as now specific phrases (subtrees) -- and in some cases individual words/nodes -- can be classified in terms of their semantic role and its "zone" status.

Such "zones" are then one step beyond the a bare-bones type-level beta reduction.  I said just before this section that I wanted to address how we can get from beta-reduction as "type-checking" for parse-graphs to actual sentence-meanings, given that (unlike possible-world semantics, say) I am not assuming that such meaning actually falls out of some computation.  Notions of type-coercion represent information "leaking" from the parse "phase" to a second "comprehension" phase.  "Zone" patterns separating out different types of semantic roles embody a differernt source of structure to help define processing and/or cognitive operations folling the gap between the two phases.


---

Given a theory of "verb detail zones", we can begin to consider the thought-processes involved as a person decides precisely how to formulate a sentence while they're speaking (by extension, the ideas which addressees must reverse engineer, or decode in revrswe, to reconstruct speakers' intensions from their projection onto purely linguistic formations).  

It is at the speaker's discretion to decide how much or how little information to supply (within limits).  For any event (etc.) profiled by a verb, it would of course be possible to name reams of situational details: the journalistic *who*, *what*, *where*, *when*, *how*, and much more.  Because, in English, evidentiality is not grammaticized, speakers are not forced to reckon wuth that branch of detailing (by contrast to tense and aspect, say) just to enunciate the correct verb-form; but of course ecidential phrases *may* be introduced, so a speaker can always qualify their comments with "it seems that", "I was told", "I saw that", etc.  Passing from the verb to its immediate neighbors, the subject and (sometimes) object(s) have to be accounted for, even if they aren't actually spoken (the subject of an imperative; the "here" in "I brought coffee").  Words or phrases have to fill in the slots of the verb's relevant conceptual matrix.  Further outward, some added detail might actually be essnetial to the sentence's premise to begin with, why the speaker has reason to say it (as in the "ice-chest cooler" for the beers).  Some extra phrases may thereby seem to be ccc [mandated] by the situation at hand.  And then there's extra content a speaker might include for clarification, or parenthetically, or as a bridge to the next sentence.

These are "rules", then, for filling in verb-related details.  Some details are mandated by morphosyntactic convention; others by verbs' "theta grid"; others by discourse priorities against the relvant situational backdrop -- if we take Grician conversational norms as imposing production-rules equal in force to syntax or semantics, then maxims such as "each sentence should present new information" would compel some detailing that would be seemingly optional, if abstracted from the current scenario around the conversation.  And some details *are* optional.  Guided by such paradigms, a speaker will embellish verbs with a succession of words or phrases that each contribute information vis-a-vis the verb in isolation from another, branching into different "zones" (in the sense I'm employing that term here), different theta-roles or TAME criteria, or different thematic relations.  

Figuratively, rather than visualizing beta-reduction as a state-propagation that collapses toward the root, condensing the a single propositional type (assuming the sentence is well-formed), this "productive" model of the sentence branches out tree-like in the opposite direction, starting from the verb-root and "growing" new branches (which in turn might have their own verbs -- cf. "the dog who you saw yesterday" as a noun-phrase for, say, "our neighbors adopted") so that the productive process repeats iteratively (factorially, even).  

At one level here I'm just evoking language's compositional creativity.  But notice that every production outward from the verb root still has to obey parameters that can be modeled in terms of beta-reduction in the opposite direction (*toward* the verb).  That is, type-level beta-reduction captures criteria that serve as guardrails shaping the production of new phrasal branches supplying new details to the verb.

Meanwhile, the source material for successive verb-qualifications isn't purely linguistic; it's the totality of situation context, which unfolds as the sentence-production itself occurs.  A speaker forms an intent to create a sentence with a specific meaning in light of the particulars that define the scenario immediately prior to the enunciation (which includes what the current speaker, or others, have said before).  The contours of a sentence can indeed change during the course of its production, insofar as the speaker might observe addressees' reactions (or respond to unanticipated extralinguistic goings-on).  

By this account, sentence-production is a dynamic process shaped by the interplay of two tendencies: an outward-expand and situation-sensitive accumulation of verb details whose linguistic articulation begins with a verb root and branches out, from the verb's own inflections to surrounding words and then phrases; and, cutting in the opposite direction, an inward pull which constrains phrase-production accoring to rules preserving how the sentence as a whole "reduces", when fully understood, to a single proposition.     
  
         



..

In and if itself beta-reduction is banal; it basically reinforce conventions that the "meaning" of, say, fx is the "result" of applying f to x.  What makes "beta-reduction modulo topology" a non-trivial paradigm, as I see it, is the "modulo topology part": the idea that some version of beta-reduction (defined, say, via propagating state-changes) exists for a variety of underlying graph forms, recognizing extra structural parameters both in the domain of computer programming languages (objects, channels, Petri nets) and natural human language (qualifier scope, word-order consideraitons, link-grammar labels, verb-detail "zones" as I've proposed them here).  Beta-reduction is thereby a formal subsystem which can be embedded in more complex models of syntax and semantics.  The "minimal" structure for this reduction is that of term algebra, which -- coupled with a policy of quantifying node-states over types rather than type-instances -- is, I have claimed, sufficient to capture a lot of details about grammatical acceptability and also (by merging coarse type-models in terms of grammatical categories with more granular type systems recognizing "ontological" categories and related theta roles) lay the groundword for sentences' preliminary semantic interpretations.  

The possibility of a type system straddling the syntax/semantics line (not confined to part-of-speech alone) amounts to modeling a larger area within formal semantics via a type theory that also models a lot of syntax.  Joining a linguistic type theory to theta roles (in the sense that "meso"-types are criteria on role instantiators in the same way that coarser types are criteria on part-of-speech) -- which to me is a reasonable gloss on the language/type-theory literature -- represents a "semanticization" of types analogous to expressive type theories in programming language implementation.  Cf. the popular sentiment in functional compiler theory that "type checking is program verification".
 [https://www.cis.upenn.edu/~mhnaik/papers/toplas08.pdf]

In all these variations, beta-reduction is a *model* for human cognitive processes; the point is not that our human minds are built around some sort of Turing machine -- some formal system that can mechanically compute over term algebras -- but rather that beta-reduction obliquely models cognitive activity attempting to capture only broad functional tendencies, not a detailed replication of neuronal patterns.  This reflects a broadly "functionalist" paradigm in the philosophy of mnid, wherein (maybe artificially) intelligent are compared based on their functional operation, not their inner workings; e.g., juxtaposing machine translation of a natural-language text to the work of a professional human translator.  The more the results match, the more that the person and the software are behaving in functionally equivalent ways, at least with respect to that one specific task.

During the productive process of creating a sentence, then, a speaker -- on this theory -- iteratively builds outward from a root verb, selecting situational details while being nudged by lexical and morphosyntactic conventions that demand attention to phenomena such as tense/aspect (to yield the verb's proper inflected form) and thematic relations (those are that definitionally intrinsic to the verb's meaning).  Surrounding these "nudges", situational details and dialogic maxims -- e.g., always presenting new information, and keeping track of which refential expressions are already established in the conversation; "a dog" introduced de novo can become "that dog" or "she" later -- motivate speakers to tack on further details as well via subordinate phrasing, yielding a chain of phrases that branches outward.  If we see this result of this process just as a static hierarchy, we might see this as a fairly mundane iterative phenomenon where some canonical theory of phrase structure (X-bar, say) simply simply gets (re-)iterated within nested phrases.  But my point is that the finished form of a phrase hierachy evolves as a dynamic process guided by the speaker enmeshed in situations, and situational details are select and filtered to sculpt the information that a sentence conveys.  The cognitive faculties behind that actual *mmeaning" of a sentence are those of speaker producing language in real time, foregrounding certain situational details that seem pertinent to the current dialog and enactive goals.  Linguistic conventions provide rules for how such details may be encoded in speech, and in this sense they partly determine linguistic content (e.g., grammaticalization of tense and aspect requires that the temporal position and extent of states/events/processes are explicitly attended to from among the overall space of situational details).  On the other hand, presumably a large part of our situational reasoning is extra-linguistic and in that sense we should not expect the relevant cognitive processes to be analyzable in linguistic terms.  Formal methods such as type-level beta reduction or theta-role type coercions will hopefully capture in indirect, functionalistic terms the processing which governs how langauge encodes situational accounts, but -- in contrast to the aspirations of possible-world semantics and predicate-logic sentence models, say -- we should not expect these formal subsystems to simulate most of *meaning* per se.

In short, compositionality is a dynamic, not static, phenomnenon.  Producing suborinate clauses involves repeating the general form of branching patterns on the root verb in a nested context, but this is not just a formal repetition of production rules away from the root, but a cognitive process wherein the speaker must keep track of the root-verb's conceptual details while temporarily focusing on other aspects of the situation.  As a purely formal matter the structure of a subtree can be modeled in isolation from the tree overall, but as an intellectual facility we cannot assume that placing attentional focus on a more granular detail causes more macroscopic conceptualizations to go away; they just get temporarily overlaid.  In "I heard you brought treats for K'eyush in your purse" the overarching situation presumably involving two people and a dog gets background when attention switches, at the end of the sentence, to "the purse" -- this might be an actual shift of perceptual focus, if the speaker actually looks toward said purse, though it could be merely a morphism in the conceptual foreground.  But the larger situation sustains a kind of retentional presence during the finer-grained episode and presumably is poised to return as attentional focus once the speaker (literally or figurative) looks away from the purse and back toward the addressee, or the dog, or the room, or whatever.  Theoretical accounts of these attentional modulations would presumably come from cognitive science and/or phenomenology, not form linguistics.  But attentional patterning leaves a *trace* within language due to production rules, and we can use formal models such as type-level beta-reduction to outline how patterns of cognitive attention become mirrored in phrase structure.


At what level are speakers "computing" them?  Perhaps there are possibilities to model transitions in cognitive-perceptual focus in computational terms, against the basis of structural theories for perceptual stimuli, or the visual field, or the transcendental unity of apperception.  Software-based image analysis and Computer Vision, for example, yields numerous algorithms which usefully simulate certain processes through which humans (and other animals, presumably) navigate through the visual field, shifting focus across larger and smaller sites connected mereologically (by analogy, image-segmentation yields finer or coarser region-sets depending on the contour-detection threshold).  Of course, enactive cognition within situations involves a lot more than vision alone, but the study of visual consciousness is a good proxy for situational embodiment in general.

Lingusitically, however, computational models of situational reasoning are only tangential to how situational details are encoded in language.  What we *can* do is model the "interface" between situational cognition and language production, and at least parts of that cognitive bridge-work appear to be formally tractable, at least to an explatorily useful extent.  In that sense, employing cognitive proceses modeled via beta-reduction or theta-grids can indeed be regarded as "computing" over these structures.  Here the "language as computation" metaphor is appropriate.  We should not overestimate the scope, however, of the regions within language specifically, or cogntion and consciousness in general, where the metaphor gives us intuitions that are useful rather than oversimplifying and reductionistic.   


      








     





Suppose someone says one of  

I brought K'eyush some treats here
I brought K'eyush some treats over there


      




 

     

      


   

 




 





he gestured. toward the door.
he gestured toward the door.

1 - "toward the foor" modifies the proposition "he gestured"
2 - "the door" is the direct object of compound verb "gesture toward"
3 - "toward the foor" is a locative clause modifying the verb "gesture"

1 - ((toward (the door))(gestured he))
2 - ((gestured toward) he (the door))
3 - (((toward (the door)) gestured) he)






he gestured toward the window.
he pointed toward the window.
he pointed out the window.
he pointed.  out the window.
How did he let us know about the new car?  He just pointed.  Out the window.



Language and Computation: The cognitive dimensions of a mathematical metaphor




He went off toward the store

went off he toward the store

toward the store went off he 

went he off toward the store

off toward the store went he 



library =
I'm heading uot {to the store}




I walked by that newspaper on the way to work
That newspaper has published many op-eds about Biden's campaign
That newspaper is on the table on top of your keys


I walked by that newspaper, down the stairs, and onto the train


I bought the newspaper for $5 (transitive or ditransitive)
I bought the newspaper this morning

I bought the newspaper for $5  vs.  Some corporation bought the newspaper for $50 million

mesotypes vs deep case

"democrats" can be anyone registered as a democratic; voters who usually select that party's candidates rather than alternatives; those who generally endorse democratic platforms and policies; political candidates themselves running as the party's official nominee and appearing in that row on ballots; elected officials who are members of the party; senators (like Bernie Sanders) who "caucus with" democratics for legistlative and procedural decisions and negotiations, etc.


lexical, morphosyntactic, and intonational choices that nudge addressees' reception pragmatics in one direction or another
    








 











